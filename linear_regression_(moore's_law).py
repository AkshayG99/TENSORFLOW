# -*- coding: utf-8 -*-
"""linear regression (moore's law)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mAgTg3_Mf1j5bY3px4MDRVboIcEi37TW
"""

#Linear Regression

#import libraries
import tensorflow as tf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

#get the data
!wget https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/tf2.0/moore.csv

!head moore.csv

# load in the data and convert to numpy array
data = pd.read_csv("moore.csv", header=None).to_numpy()

data

#split x and y
#will assume x is 2d array
X = data[:,0].reshape(-1,1) # making a N x D matrix
Y = data[:,1]

#plot data
plt.scatter(X, Y);

#make graph linear for linear regression
Y = np.log(Y)
plt.scatter(X, Y)

#Shift data down by its mean to make it have mean of 0
X = X - X.mean()

#build model
model = tf.keras.models.Sequential([
    tf.keras.layers.Input(shape=(1,)),
    tf.keras.layers.Dense(1),
])

#compile model
#first number is learning rate (0.001)
model.compile(
    optimizer=tf.keras.optimizers.SGD(0.001, 0.9),
    loss="mse"
)

#epoch=step iteration, lr = learning rate
def schedule(epoch, lr):
  if epoch >= 50:
    return 0.0001
  return 0.001

scheduler = tf.keras.callbacks.LearningRateScheduler(schedule)

#train the model
r = model.fit(X, Y, epochs=200, callbacks=[scheduler])

# Plot the loss
plt.plot(r.history["loss"], label="loss")
plt.legend();

#would usually be the end - but goal is to model
model.layers

#first array is 2d matrix and second array is list
model.layers[0].get_weights()

#get number in first array
a = model.layers[0].get_weights()[0][0,0]

a

#watch 14. regression textbook at 17:09 for math

#print the time to double (in years)
print("Time to double: ", np.log(2) / a)

#if you know linear regression
X = X.flatten()
denominator = X.dot(X) - X.mean() * X.sum()
a = (X.dot(Y) - Y.mean() * X.sum()) / denominator
b = (Y.mean() * X.dot(X) - X.mean() * X.dot(Y)) / denominator
print(a, b)

#print the time to double (in years)
print("Time to double: ", np.log(2) / a)

#MAKING PREDICTIONS
#Make sure line fits our data by drawing line
#need to flatten because tensor flow dense layer general enough to have multiple inputs you need to flattem
Yhat = model.predict(X).flatten()
plt.scatter(X, Y)
plt.plot(X, Yhat)

# manual calculations to ensure dense layer does linear transformation (w dot x + b)

#get the weights
w, b = model.layers[0].get_weights()

#Reshape X because we flattended it before - need to make it 2d again
X = X.reshape(-1, 1)

# (N x 1) x (1 x 1) + (1) --> (N x 1)
# X is a n x 1 array and w is a 1 x 1 array
# result of multiplcation is N x 1 adding B as length vector so same dimension
# flatten so that we get a 1 dimensional array
Yhat2 = (X.dot(w) + b).flatten()

#check that Yhat is equal to Yhat2
#dont use == for floats
#np.allclose checks difference
np.allclose(Yhat, Yhat2)